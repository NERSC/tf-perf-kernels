{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "cudadir = \"/usr/common/software/cuda/10.1.243\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and output dirs\n",
    "datadirs = [\"../scripts/tf_cnn_kernels_2/runs/345569\"]\n",
    "#datadirs = [\"../data/tf_2.0b/nsight\"]\n",
    "#outputdir = \"./results/tf_2.0b/results_NHWC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_frame(df_times, df_metrics):\n",
    "    #Copy the profile frame to make sure not to overwrite it and potentially read it in again if we screwed it up\n",
    "    selectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \"Batch Size\", \"Pass\", \"Name\"]\n",
    "    tc_peak_perf_flops = 125*10**12\n",
    "\n",
    "    #just pick the gpu activities for now\n",
    "    profiledf = df_times.copy()\n",
    "    profiledf.sort_values(by=selectkeys,inplace=True)\n",
    "    profiledf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #remove the calibration\n",
    "    alignkeys = selectkeys[:-2]\n",
    "    profiledf = profiledf.groupby(alignkeys).apply(lambda x: x[ (~x[\"Name\"].isin(x.loc[x[\"Pass\"].str.startswith(\"calibrate\"), \"Name\"].values)) ])\n",
    "    profiledf.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #as metricdf use df_summary\n",
    "    metricdf = df_metrics.copy()\n",
    "    metricdf.sort_values(by=selectkeys,inplace=True)\n",
    "    metricdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #now, get the AI-relevant stuff:\n",
    "    #FLOPS 32\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_fadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_ffma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_fmul_pred_on']\n",
    "    tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    tmpdf = tmpdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"FP32 Flops Avg\"})\n",
    "    #add to timings\n",
    "    print(\"\\nName from Nsight Systems:\\n\", profiledf[\"Name\"])\n",
    "    print(\"\\nName from Nsight Compute\\n\", tmpdf[\"Name\"])\n",
    "    profiledf = profiledf.merge(tmpdf[selectkeys+[\"FP32 Flops Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    return profiledf\n",
    "    \n",
    "    \n",
    "    #monitor that: if that changes be warned\n",
    "    numrows = profiledf.shape[0]\n",
    "\n",
    "    #FLOPS 16 non-TC\n",
    "    flopdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"flop_count_hp\") ].sort_values(selectkeys).rename(columns={\"Avg\": \"FP16 non-TC Flops Avg\"})\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(flopdf[selectkeys+[\"FP16 non-TC Flops Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        #print(profiledf, flopdf)\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "    \n",
    "    \n",
    "    #FLOPS TC\n",
    "    flopdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"tensor_precision_fu_utilization\") ].sort_values(selectkeys).rename(columns={\"Avg\": \"TC Flops Avg\"})\n",
    "    tmpdf = flopdf.merge(profiledf, how=\"inner\", on=selectkeys).sort_values(selectkeys)\n",
    "    tmpdf[\"TC Flops Avg\"] *= tc_peak_perf_flops/10. * tmpdf[\"Time Avg\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(tmpdf[selectkeys+[\"TC Flops Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "\n",
    "    \n",
    "    #fill NA values here\n",
    "    profiledf.fillna(0., inplace=True)\n",
    "\n",
    "    #FLOPS FP16: add TC and non-TC FP16 flops together\n",
    "    profiledf[\"FP16 Flops Avg\"] = profiledf[\"TC Flops Avg\"] + profiledf[\"FP16 non-TC Flops Avg\"]\n",
    "\n",
    "    #total flops\n",
    "    profiledf[\"Flops Avg\"] = profiledf[\"FP16 Flops Avg\"] + profiledf[\"FP32 Flops Avg\"]\n",
    "\n",
    "    #flop fractions\n",
    "    profiledf[\"TC Flops Fraction Avg\"] = profiledf[\"TC Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "    profiledf[\"FP16 Flops Fraction Avg\"] = profiledf[\"FP16 Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "    profiledf[\"FP16 non-TC Flops Fraction Avg\"] = profiledf[\"FP16 non-TC Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "    profiledf[\"FP32 Flops Fraction Avg\"] = profiledf[\"FP32 Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "\n",
    "\n",
    "    #shared\n",
    "    #project out\n",
    "    shareddf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"shared\") ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    sharedreadsdf = shareddf.loc[(shareddf[\"Metric Name\"]==\"shared_transactions\") & (shareddf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "    sharedwritesdf = shareddf.loc[(shareddf[\"Metric Name\"]==\"shared_transactions\") & (shareddf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "    #combine\n",
    "    shareddf = sharedwritesdf.merge(sharedreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    shareddf[\"Shared Transactions Avg\"] = shareddf[\"Avg_x\"] + shareddf[\"Avg_y\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(shareddf[selectkeys+[\"Shared Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        #get the complement:\n",
    "        print(profiledf[ ~profiledf.index.isin(mergedf.index) ])\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "    \n",
    "    \n",
    "    #atomic\n",
    "    #project out\n",
    "    atomicdf = metricdf[ metricdf[\"Metric Name\"] == \"atomic_transactions\" ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    atomicdf = atomicdf[selectkeys+[\"Avg\"]].rename(columns={\"Avg\": \"Atomic Transactions Avg\"})\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(atomicdf[selectkeys+[\"Atomic Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        #get the complement:\n",
    "        print(profiledf[ ~profiledf.index.isin(mergedf.index) ])\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "\n",
    "    \n",
    "    #L1\n",
    "    #project out\n",
    "    l1df = metricdf[ (metricdf[\"Metric Name\"].str.contains(\"gst_\")) | (metricdf[\"Metric Name\"].str.contains(\"gld_\")) ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    l1readsdf = l1df.loc[(l1df[\"Metric Name\"]==\"gld_transactions\"), selectkeys+[\"Avg\"]]\n",
    "    l1writesdf = l1df.loc[(l1df[\"Metric Name\"]==\"gst_transactions\"), selectkeys+[\"Avg\"]]\n",
    "    #combine\n",
    "    l1df = l1writesdf.merge(l1readsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    l1df[\"L1 Transactions Avg\"] = l1df[\"Avg_x\"] + l1df[\"Avg_y\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(l1df[selectkeys+[\"L1 Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        print(profiledf, l1df)\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "\n",
    "    \n",
    "    #L2\n",
    "    #project out\n",
    "    l2df = metricdf[ metricdf[\"Metric Name\"].str.contains(\"l2\") ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    l2readsdf = l2df.loc[(l2df[\"Metric Name\"]==\"l2_transactions\") & (l2df[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "    l2writesdf = l2df.loc[(l2df[\"Metric Name\"]==\"l2_transactions\") & (l2df[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "    #combine\n",
    "    l2df = l2writesdf.merge(l2readsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    l2df[\"L2 Transactions Avg\"] = l2df[\"Avg_x\"] + l2df[\"Avg_y\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(l2df[selectkeys+[\"L2 Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        print(profiledf, l2df)\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "    \n",
    "    \n",
    "    #DRAM\n",
    "    #project out\n",
    "    dramdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"dram\") ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    dramreadsdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram_transactions\") & (dramdf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "    dramwritesdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram_transactions\") & (dramdf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "    #combine\n",
    "    dramdf = dramwritesdf.merge(dramreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    dramdf[\"DRAM Transactions Avg\"] = dramdf[\"Avg_x\"] + dramdf[\"Avg_y\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(dramdf[selectkeys+[\"DRAM Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        print(profiledf, dramdf)\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "    \n",
    "\n",
    "    #SYSMEM\n",
    "    #project out\n",
    "    sysmemdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"sysmem\") ].sort_values(selectkeys)\n",
    "    #get reads and writes\n",
    "    sysmemreadsdf = sysmemdf.loc[(sysmemdf[\"Metric Name\"]==\"sysmem_transactions\") & (sysmemdf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "    sysmemwritesdf = sysmemdf.loc[(sysmemdf[\"Metric Name\"]==\"sysmem_transactions\") & (sysmemdf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "    #combine\n",
    "    sysmemdf = sysmemwritesdf.merge(sysmemreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    sysmemdf[\"Sysmem Transactions Avg\"] = sysmemdf[\"Avg_x\"] + sysmemdf[\"Avg_y\"]\n",
    "    #add to timings\n",
    "    mergedf = profiledf.merge(sysmemdf[selectkeys+[\"Sysmem Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    #check\n",
    "    if mergedf.shape[0] != numrows:\n",
    "        print(profiledf, sysmemdf)\n",
    "        raise ValueError(\"Something went wrong, check consistency of inputs\")\n",
    "    else:\n",
    "        profiledf = mergedf\n",
    "    \n",
    "\n",
    "    #clean up and sort:\n",
    "    profiledf.sort_values(selectkeys).reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #get performance first\n",
    "    profiledf[\"Performance GFlop/s\"] = profiledf[\"Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "    profiledf[\"FP32 Performance GFlop/s\"] = profiledf[\"FP32 Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "    profiledf[\"FP16 Performance GFlop/s\"] = profiledf[\"FP16 Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "    profiledf[\"TC Performance GFlop/s\"] = profiledf[\"TC Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "\n",
    "    #get AI:\n",
    "    #L1 is L1+shared\n",
    "    profiledf[\"L1 AI\"] = profiledf[\"Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]+profiledf[\"Atomic Transactions Avg\"]))\n",
    "    profiledf[\"FP32 L1 AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]+profiledf[\"Atomic Transactions Avg\"]))\n",
    "    profiledf[\"FP16 L1 AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]+profiledf[\"Atomic Transactions Avg\"]))\n",
    "    #L2\n",
    "    profiledf[\"L2 AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "    profiledf[\"FP32 L2 AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "    profiledf[\"FP16 L2 AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "    #DRAM\n",
    "    profiledf[\"DRAM AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "    profiledf[\"FP32 DRAM AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "    profiledf[\"FP16 DRAM AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "    #Sysmem\n",
    "    profiledf[\"Sysmem AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "    profiledf[\"FP32 Sysmem AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "    profiledf[\"FP16 Sysmem AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "\n",
    "    #sort results\n",
    "    profiledf.sort_values(by=selectkeys).reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return profiledf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the files\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ os.path.join(datadir,x) for x in os.listdir(datadir) if ((os.path.splitext(x)[-1] == \".nsight-cuprof-report\")\n",
    "                                                                        or (os.path.splitext(x)[-1] == \".sqlite\"))]\n",
    "\n",
    "#recs\n",
    "records = []\n",
    "\n",
    "#build feature list:\n",
    "for path in files:\n",
    "    \n",
    "    #filename\n",
    "    file = os.path.basename(path)\n",
    "    \n",
    "    #path\n",
    "    path = os.path.dirname(path)\n",
    "    \n",
    "    #splitup\n",
    "    splt = file.split(\".\")\n",
    "    \n",
    "    prefix = \".\".join(splt[0:-2])\n",
    "    \n",
    "    #append to records\n",
    "    records.append({\"prefix\": prefix, \"file\": os.path.join(path, file)})\n",
    "\n",
    "#put in df\n",
    "recorddf = pd.DataFrame(records).sort_values([\"prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name from Nsight Systems:\n",
      " 0                          cudnnConvolutionBackwardData\n",
      "1                        cudnnConvolutionBackwardFilter\n",
      "2                               cudnnConvolutionForward\n",
      "3     void Eigen::internal::EigenMetaKernel<Eigen::T...\n",
      "4     void cudnn::detail::convolve_common_engine_flo...\n",
      "5     void cudnn::detail::convolve_common_engine_flo...\n",
      "6     void cudnn::detail::wgrad_alg0_engine_NHWC<__h...\n",
      "7     void foldedNhwcToNhwcKernel<__half, __half, fl...\n",
      "8     void nhwcToFoldedNhwcKernel<__half, __half, fl...\n",
      "9     void tensorflow::functor::PadInputCustomKernel...\n",
      "10    void tensorflow::functor::ShuffleInTensor3Simp...\n",
      "11    void tensorflow::functor::ShuffleInTensor3Simp...\n",
      "12                              cudnnConvolutionForward\n",
      "13    void cudnn::detail::convolve_common_engine_flo...\n",
      "14    void tensorflow::functor::PadInputCustomKernel...\n",
      "15    void tensorflow::functor::ShuffleInTensor3Simp...\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Name from Nsight Compute\n",
      " 0                       EigenMetaKernel\n",
      "1          FillPhiloxRandomKernelLaunch\n",
      "2              PadInputCustomKernelNHWC\n",
      "3                ShuffleInTensor3Simple\n",
      "4     convolve_common_engine_float_NHWC\n",
      "5                foldedNhwcToNhwcKernel\n",
      "6                nhwcToFoldedNhwcKernel\n",
      "7                wgrad_alg0_engine_NHWC\n",
      "8                       EigenMetaKernel\n",
      "9          FillPhiloxRandomKernelLaunch\n",
      "10                      EigenMetaKernel\n",
      "11         FillPhiloxRandomKernelLaunch\n",
      "12             PadInputCustomKernelNHWC\n",
      "13               ShuffleInTensor3Simple\n",
      "14    convolve_common_engine_float_NHWC\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#sort by those keys:\n",
    "sortkeys = [\"Network Name\", \"Input Shape\", \"Kernel Shape\", \\\n",
    "            \"Batch Size\", \"Stride Size\", \"Data Format\", \"Pass\", \\\n",
    "            \"Precision\", \"Device\", \"Name\", \"Metric Name\"]\n",
    "\n",
    "#limit the input\n",
    "#recorddf = recorddf[ recorddf[\"prefix\"].str.startswith(\"profile.name_ResNet50-2.batchsize_16.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32\") ]\n",
    "\n",
    "#group by prefixes and files\n",
    "all_prefixes = set([x.split(\".pass\")[0] for x in recorddf[\"prefix\"]])\n",
    "all_passes = set([x.split(\".pass_\")[1].replace(\".pass_\",\"\") for x in recorddf[\"prefix\"].unique()])\n",
    "\n",
    "#print(recorddf.values[0])\n",
    "\n",
    "#metrics\n",
    "df_profiles = []\n",
    "\n",
    "for pref in all_prefixes:\n",
    "    \n",
    "    #set empty lists\n",
    "    df_times = []\n",
    "    df_timeline = []\n",
    "    df_summary = []\n",
    "    \n",
    "    #print prefix\n",
    "    #print(pref)\n",
    "    \n",
    "    #loop over passes\n",
    "    df_times = []\n",
    "    df_metrics = []\n",
    "    for pas in all_passes:\n",
    "        \n",
    "        #project frame\n",
    "        files = recorddf.loc[ recorddf[\"prefix\"] == pref + \".pass_\" + pas, \"file\" ].values\n",
    "        \n",
    "        #project the invididual files\n",
    "        timefile = [x for x in files if x.endswith(\".sqlite\")][0]\n",
    "        metricfile = [x for x in files if x.endswith(\".nsight-cuprof-report\")][0]\n",
    "        \n",
    "        #get the parameters from the filename\n",
    "        parameters, _ = parse_filename(os.path.basename(metricfile))\n",
    "        \n",
    "        #metrics\n",
    "        metricdf = import_nsight_metric(metricfile, cuda_dir=cudadir)\n",
    "        for key in parameters:\n",
    "                metricdf[key] = parameters[key]\n",
    "        \n",
    "        #fuse read/write metrics together:\n",
    "        unique_metrics = metricdf[\"Metric Name\"].unique()\n",
    "        unique_metrics = set([x.replace(\".sum\",\"\").replace(\"_write\",\"\").replace(\"_read\",\"\").replace(\"_ld\",\"\").replace(\"_st\",\"\") for x in unique_metrics])\n",
    "        #add the metric type\n",
    "        metricdf[\"Metric Type\"] = \"total\"\n",
    "        #read\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_read\"), \"Metric Type\" ] = \"read\"\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_ld\"), \"Metric Type\" ] = \"read\"\n",
    "        #write\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_write\"), \"Metric Type\" ] = \"write\"\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_st\"), \"Metric Type\" ] = \"write\"\n",
    "        \n",
    "        for metric in unique_metrics:\n",
    "            metricdf.loc[ metricdf[ \"Metric Name\"].str.startswith(metric), \"Metric Name\" ] = metric\n",
    "            \n",
    "        #import time\n",
    "        timedf = import_nsight_overview(timefile)\n",
    "        for key in parameters:\n",
    "                timedf[key] = parameters[key]\n",
    "                \n",
    "        #append to DF:\n",
    "        df_times.append(timedf)\n",
    "        df_metrics.append(metricdf)\n",
    "    \n",
    "    timedf = pd.concat(df_times)\n",
    "    metricdf = pd.concat(df_metrics)\n",
    "    \n",
    "    #compute the profile\n",
    "    profiledf = transpose_frame(timedf, metricdf)\n",
    "    #print(profiledf)\n",
    "    break\n",
    "        \n",
    "    #    #loop over metrics\n",
    "    #    for met in [x for x in all_metrics if x != \"time\"]:\n",
    "    #        \n",
    "    #        #filename\n",
    "    #        file = selectdf.loc[ selectdf[\"metric\"] == met, \"file\" ].values[0]\n",
    "    #    \n",
    "    #        #extract metric name\n",
    "    #        parameters, metric = parse_filename(os.path.basename(file))\n",
    "    #        metrics = metric.split(\"-\")\n",
    "    # \n",
    "    #        #import as timeline\n",
    "    #        tmpdf = import_nvprof_metric(file, timeline=True, cuda_dir=cudadir)\n",
    "    #        for key in parameters:\n",
    "    #            tmpdf[key] = parameters[key]\n",
    "    #    \n",
    "    #        #replace \"Idle (0)\" with 0.:\n",
    "    #        for metric in metrics:\n",
    "    #            if metric==\"tensor_precision_fu_utilization\":\n",
    "    #                tmpdf[metric] = tmpdf[metric].apply(lambda x: replace_tc_string(x))\n",
    "    #\n",
    "    #        #combine read and write metrics\n",
    "    #        tmpdf = tmpdf.groupby([x for x in tmpdf.columns if x not in metrics]).apply(lambda x: combine_metrics(x, metrics)).reset_index()\n",
    "    #        lev = [x for x in tmpdf.columns if x.startswith(\"level_\")][0]\n",
    "    #        del tmpdf[lev]\n",
    "    #        df_timeline.append(tmpdf)\n",
    "    # \n",
    "    #        #import as summary\n",
    "    #        tmpdf = import_nvprof_metric(file, timeline=False, cuda_dir=cudadir).sort_values(by=\"Name\").reset_index(drop=True)\n",
    "    #        tmpdf[\"Metric Mode\"] = \"read\" if \"read\" in metric else \"write\" if \"write\" in metric else \"write\" if \"store\" in metric else \"read\" if \"load\" in metric else \"total\"\n",
    "    #        tmpdf[\"Metric Name\"] = metric.replace(\"read\",\"\").replace(\"write\",\"\").replace(\"store\",\"\").replace(\"load\",\"\").replace(\"__\",\"_\")\n",
    "    #        for key in parameters:\n",
    "    #            tmpdf[key] = parameters[key]\n",
    "    #        del tmpdf[\"Metric Description\"]\n",
    "    # \n",
    "    #        #replace \"Idle (0)\" with 0.:\n",
    "    #        for metric in metrics:\n",
    "    #            if metric==\"tensor_precision_fu_utilization\":\n",
    "    #                tmpdf[ \"Min\" ] = tmpdf[ \"Min\" ].apply(lambda x: replace_tc_string(x))\n",
    "    #                tmpdf[ \"Max\" ] = tmpdf[ \"Max\" ].apply(lambda x: replace_tc_string(x))\n",
    "    #                tmpdf[ \"Avg\" ] = tmpdf[ \"Avg\" ].apply(lambda x: replace_tc_string(x))\n",
    "    #        df_summary.append(tmpdf)\n",
    "    #    \n",
    "    #    #do time now\n",
    "    #    file = selectdf.loc[ selectdf[\"metric\"] == \"time\", \"file\" ].values[0]\n",
    "    #    timedf, markerdf = import_nvprof_overview(file, cuda_dir=cudadir)\n",
    "    #\n",
    "    #    #extract metric name\n",
    "    #    parameters, _ = parse_filename(os.path.basename(file))\n",
    "    #    for key in parameters:\n",
    "    #        timedf[key] = parameters[key]\n",
    "    #    df_times.append(timedf)\n",
    "        \n",
    "    ##concat into frame\n",
    "    #metricdf = pd.concat(df_summary, sort=True)\n",
    "    #timedf = pd.concat(df_times, sort=True)\n",
    "    \n",
    "    ##transpose\n",
    "    #profiledf = transpose_frame(timedf, metricdf)\n",
    "    #df_profiles.append(profiledf)\n",
    "\n",
    "#concat everything\n",
    "#profiledf = pd.concat(df_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dram__sectors', 'l1tex__t_sectors_pipe_lsu_mem_global_op',\n",
       "       'l1tex__t_sectors_pipe_lsu_mem_local_op',\n",
       "       'l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom',\n",
       "       'l1tex__t_set_accesses_pipe_lsu_mem_global_op_red',\n",
       "       'l1tex__t_set_accesses_pipe_tex_mem_surface_op_atom',\n",
       "       'l1tex__t_set_accesses_pipe_tex_mem_surface_op_red',\n",
       "       'lts__t_sectors_aperture_sysmem_op', 'lts__t_sectors_op',\n",
       "       'sm__inst_executed_pipe_tensor_op_hmma.avg.pct_of_peak_sustained_active',\n",
       "       'smsp__inst_executed_op_shared',\n",
       "       'smsp__sass_thread_inst_executed_op_fadd_pred_on',\n",
       "       'smsp__sass_thread_inst_executed_op_ffma_pred_on',\n",
       "       'smsp__sass_thread_inst_executed_op_fmul_pred_on',\n",
       "       'smsp__sass_thread_inst_executed_op_hadd_pred_on',\n",
       "       'smsp__sass_thread_inst_executed_op_hfma_pred_on',\n",
       "       'smsp__sass_thread_inst_executed_op_hmul_pred_on'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricdf[\"Metric Name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute AI Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Invocations</th>\n",
       "      <th>Time Avg</th>\n",
       "      <th>Network Name</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Input Shape</th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Stride Size</th>\n",
       "      <th>Data Format</th>\n",
       "      <th>Pass</th>\n",
       "      <th>Precision</th>\n",
       "      <th>FP32 Flops Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Time, Invocations, Time Avg, Network Name, Batch Size, Input Shape, Kernel Shape, Stride Size, Data Format, Pass, Precision, FP32 Flops Avg]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#profiledf[ (profiledf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (profiledf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (profiledf[\"Batch Size\"]==16) &\\\n",
    "#           (profiledf[\"Precision\"]==\"FP32\") &\\\n",
    "#           (profiledf[\"Stride Size\"]==2) &\\\n",
    "#           (profiledf[\"Pass\"]==\"forward\") &\\\n",
    "#           (profiledf[\"Kernel Shape\"]==\"7x7x64x64\")\n",
    "#         ]\n",
    "profiledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Calls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Calls'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-57fc3e0f381a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombineddf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Avg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcombineddf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcombineddf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Calls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#sum up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/common/software/python/3.7-anaconda-2019.07/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Calls'"
     ]
    }
   ],
   "source": [
    "#sum over all kernels\n",
    "combinedselectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \\\n",
    "                     \"Batch Size\", \"Pass\"]\n",
    "\n",
    "#copy profiledf\n",
    "combineddf = profiledf.copy()\n",
    "\n",
    "#get the aggregated performance, including all kernels:\n",
    "#compute weights: multiply all measures by the number of invocations\n",
    "weighted = True\n",
    "if weighted:\n",
    "    #first, get all the names of metrics which need to be weighted\n",
    "    metrics = [x for x in combineddf.columns if \"Avg\" in x]\n",
    "    for metric in metrics:\n",
    "        combineddf[metric] *= combineddf[\"Calls\"]\n",
    "    \n",
    "#sum up\n",
    "combineddf = combineddf.groupby(by=combinedselectkeys).sum()\n",
    "\n",
    "#the flop fractions need to be recomputed\n",
    "combineddf[\"TC Flops Fraction Avg\"] = combineddf[\"TC Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP16 Flops Fraction Avg\"] = combineddf[\"FP16 Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP16 non-TC Flops Fraction Avg\"] = combineddf[\"FP16 non-TC Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP32 Flops Fraction Avg\"] = combineddf[\"FP32 Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "\n",
    "#get performance first\n",
    "combineddf[\"Performance GFlop/s\"] = combineddf[\"Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"FP32 Performance GFlop/s\"] = combineddf[\"FP32 Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"FP16 Performance GFlop/s\"] = combineddf[\"FP16 Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"TC Performance GFlop/s\"] = combineddf[\"TC Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "\n",
    "#get AI:\n",
    "#L1 is L1+shared\n",
    "combineddf[\"L1 AI\"] = combineddf[\"Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]+combineddf[\"Atomic Transactions Avg\"]))\n",
    "combineddf[\"FP32 L1 AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]+combineddf[\"Atomic Transactions Avg\"]))\n",
    "combineddf[\"FP16 L1 AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]+combineddf[\"Atomic Transactions Avg\"]))\n",
    "combineddf[\"TC L1 AI\"] = combineddf[\"TC Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]+combineddf[\"Atomic Transactions Avg\"]))\n",
    "#L2\n",
    "combineddf[\"L2 AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"FP32 L2 AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"FP16 L2 AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"TC L2 AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "#DRAM\n",
    "combineddf[\"DRAM AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"FP32 DRAM AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"FP16 DRAM AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"TC DRAM AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "#Sysmem\n",
    "combineddf[\"Sysmem AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"FP32 Sysmem AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"FP16 Sysmem AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"TC Sysmem AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "\n",
    "#print\n",
    "combineddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combineddf)\n",
    "# combineddf.keys\n",
    "# combineddf.columns\n",
    "\n",
    "# combineddf['Name']\n",
    "# combineddf.iloc[0,1]\n",
    "# combineddf.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineddf = combineddf.reset_index()\n",
    "#seldf = combineddf[ (combineddf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (combineddf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (combineddf[\"Precision\"]==\"FP32\")]\n",
    "#seldf\n",
    "#combineddf[[\"FP32 L2 AI\", \"FP32 L1 AI\"]]\n",
    "combineddf[[\"L2 AI\", \"L1 AI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiledf.to_csv(os.path.join(outputdir,\"full_profile.csv\"))\n",
    "combineddf.to_csv(os.path.join(outputdir,\"combined_profile.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
