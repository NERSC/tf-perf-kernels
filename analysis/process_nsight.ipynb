{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "cudadir = \"/usr/common/software/cuda/10.2.89\"\n",
    "homedir = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and output dirs\n",
    "datadirs = [\"../scripts/tf_cnn_kernels_nsight/runs/386219\"]\n",
    "#datadirs = [\"../scripts/tf_cnn_kernels_nsight/runs/386058\"]\n",
    "#datadirs = os.path.join(homedir,\"data/tf_2.0b/nsight\"]\n",
    "outputdir = \"../results/tf2_nsight/results_NHWC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_frame(df_metrics):\n",
    "    #Copy the profile frame to make sure not to overwrite it and potentially read it in again if we screwed it up\n",
    "    selectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \"Batch Size\", \"Pass\", \"Name\"]\n",
    "    \n",
    "    tc_peak_perf_flops = 125*10**12\n",
    "\n",
    "    #as metricdf use df_summary\n",
    "    metricdf = df_metrics.copy()\n",
    "    metricdf.sort_values(by=selectkeys,inplace=True)\n",
    "    metricdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #remove the calibration\n",
    "    metricdf = metricdf[metricdf[\"Pass\"] != \"calibrate\"]\n",
    "    \n",
    "    #tmp = [metricdf[\"Metric Name\"].unique()]\n",
    "    #print(tmp)\n",
    "    \n",
    "    \n",
    "\n",
    "    ####### Get timing information\n",
    "\n",
    "    ### CUDA Time\n",
    "    cudatimedf = metricdf[ (metricdf[\"Metric Name\"].str.contains(\"smsp__cycles_elapsed\")) ].sort_values(selectkeys)\n",
    "    # get cycles and rates\n",
    "    cyclesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__cycles_elapsed\") & (metricdf[\"Metric Type\"]==\"total\"), selectkeys+[\"Metric Value\"]]\n",
    "    ratesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__cycles_elapsed\") & (metricdf[\"Metric Type\"]==\"rate\"), selectkeys+[\"Metric Value\"]]\n",
    "    # combine\n",
    "    cudatimedf = cyclesdf.merge(ratesdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    cudatimedf[\"CUDA Time Avg\"] = cudatimedf[\"Metric Value_x\"] / (cudatimedf[\"Metric Value_y\"] * 1e9)\n",
    "    cudatimedf = cudatimedf.fillna(0.)\n",
    "    # merge into results\n",
    "    metricdf = metricdf.merge(cudatimedf[selectkeys+[\"CUDA Time Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### Tensor Core Time\n",
    "    tctimedf = metricdf[ (metricdf[\"Metric Name\"].str.contains(\"smsp__pipe_tensor_op_hmma_cycles_active\")) ].sort_values(selectkeys)\n",
    "    # get cycles and rates\n",
    "    cyclesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__pipe_tensor_op_hmma_cycles_active\") & (metricdf[\"Metric Type\"]==\"total\"), selectkeys+[\"Metric Value\"]]\n",
    "    ratesdf = metricdf.loc[(metricdf[\"Metric Name\"]==\"smsp__pipe_tensor_op_hmma_cycles_active\") & (metricdf[\"Metric Type\"]==\"rate\"), selectkeys+[\"Metric Value\"]]\n",
    "    # combine\n",
    "    tctimedf = cyclesdf.merge(ratesdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    tctimedf[\"TC Time Avg\"] = tctimedf[\"Metric Value_x\"] / (tctimedf[\"Metric Value_y\"] * 1e9).fillna(0.)\n",
    "    tctimedf = tctimedf.fillna(0.)\n",
    "    # merge into results\n",
    "    metricdf = metricdf.merge(tctimedf[selectkeys+[\"TC Time Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    ### check\n",
    "    tmpdf = metricdf.loc[(abs(metricdf[\"CUDA Time Avg\"] - metricdf[\"TC Time Avg\"])/metricdf[\"CUDA Time Avg\"] > 0.01) & (metricdf[\"TC Time Avg\"] != 0)]\n",
    "    if not tmpdf.empty:\n",
    "        print(tmpdf)\n",
    "        raise ValueError(\"CUDA Time not consistent wit TC Time\") \n",
    "        \n",
    "        \n",
    "        \n",
    "    ####### Get number of FLOPs\n",
    "    \n",
    "    ### FMA FLOPs = number of FMA instructions x 2\n",
    "    metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"fma\"), [\"Metric Value\"]] *= 2\n",
    "    \n",
    "\n",
    "    ### FP64 FLOPs\n",
    "    #metrics = ['smsp__sass_thread_inst_executed_op_dadd_pred_on',\n",
    "    #           'smsp__sass_thread_inst_executed_op_dfma_pred_on',\n",
    "    #           'smsp__sass_thread_inst_executed_op_dmul_pred_on']\n",
    "    #tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    #tmpdf = tmpdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"FP64 FLOPs\"})\n",
    "    #metricdf = metricdf.merge(tmpdf[selectkeys+[\"FP64 FLOPs\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### FP32 FLOPs\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_fadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_ffma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_fmul_pred_on']\n",
    "    tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    tmpdf = tmpdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"FP32 FLOPs Avg\"})\n",
    "    metricdf = metricdf.merge(tmpdf[selectkeys+[\"FP32 FLOPs Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### FP16 FLOPs\n",
    "    metrics = ['smsp__sass_thread_inst_executed_op_hadd_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_hfma_pred_on',\n",
    "               'smsp__sass_thread_inst_executed_op_hmul_pred_on']\n",
    "    tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    tmpdf = tmpdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"FP16 FLOPs Avg\"})\n",
    "    #print(tmpdf)\n",
    "    metricdf = metricdf.merge(tmpdf[selectkeys+[\"FP16 FLOPs Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### TC FLOPs\n",
    "    tmpdf = metricdf.loc[ metricdf[\"Metric Name\"].str.contains(\"tensor_op_hmma.avg.pct_of_peak\"), selectkeys+[\"TC Time Avg\", \"Metric Value\", \"Invocations\"] ].copy()\n",
    "    tmpdf[\"Utilization\"] = 0.01 * tmpdf[\"Metric Value\"] / tmpdf[\"Invocations\"]\n",
    "    #print(tmpdf)\n",
    "    tmpdf[\"TC FLOPs Avg\"] = tc_peak_perf_flops * tmpdf[\"Utilization\"] * tmpdf[\"TC Time Avg\"]\n",
    "    # merge\n",
    "    metricdf = metricdf.merge(tmpdf[selectkeys+[\"TC FLOPs Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    \n",
    "    ### Total FLOPs\n",
    "    metricdf[\"FLOPs Avg\"] = metricdf[\"FP32 FLOPs Avg\"] + metricdf[\"FP16 FLOPs Avg\"] + metricdf[\"TC FLOPs Avg\"] #+ metricdf[\"FP64 FLOPs\"]\n",
    "\n",
    "    \n",
    "    ### FLOPs fractions\n",
    "    #metricdf[\"FP64 FLOPs Fraction\"] = metricdf[\"FP64 FLOPs\"]/metricdf[\"FLOPs\"]\n",
    "    metricdf[\"FP32 FLOPs Fraction Avg\"] = metricdf[\"FP32 FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    metricdf[\"FP16 FLOPs Fraction Avg\"] = metricdf[\"FP16 FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    metricdf[\"TC FLOPs Fraction Avg\"]   = metricdf[\"TC FLOPs Avg\"]/metricdf[\"FLOPs Avg\"]\n",
    "    #print(metricdf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Get number of bytes\n",
    "    \n",
    "    ### Shared transactions\n",
    "    #project out\n",
    "    shareddf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"smsp__inst_executed_op_shared\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    shareddf = shareddf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"Shared Transactions Avg\"})\n",
    "    #add to timings\n",
    "    metricdf = metricdf.merge(shareddf[selectkeys+[\"Shared Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "    \n",
    "    ### L1 atomic transactions\n",
    "    # project out\n",
    "    metrics = ['l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom',\n",
    "               'l1tex__t_set_accesses_pipe_lsu_mem_global_op_red',\n",
    "               'l1tex__t_set_accesses_pipe_tex_mem_surface_op_atom',\n",
    "               'l1tex__t_set_accesses_pipe_tex_mem_surface_op_red']\n",
    "    atomicdf = metricdf.loc[ metricdf[\"Metric Name\"].isin(metrics), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    # get reads and writes\n",
    "    atomicdf = atomicdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"L1 Atomic Transactions Avg\"})\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(atomicdf[selectkeys+[\"L1 Atomic Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### Local transactions \n",
    "    # project out\n",
    "    localdf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"l1tex__t_sectors_pipe_lsu_mem_local_op\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    localdf = localdf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"Local Transactions Avg\"})\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(localdf[selectkeys+[\"Local Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### Global transactions \n",
    "    # project out\n",
    "    globaldf = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"l1tex__t_sectors_pipe_lsu_mem_global_op\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    globaldf = globaldf.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"Global Transactions Avg\"})\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(globaldf[selectkeys+[\"Global Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### L1 Bytes\n",
    "    metricdf[\"L1 Bytes Avg\"] = (metricdf[\"Shared Transactions Avg\"] + metricdf[\"L1 Atomic Transactions Avg\"]\n",
    "                            + metricdf[\"Local Transactions Avg\"] + metricdf[\"Global Transactions Avg\"]) * 32\n",
    "    # clean up\n",
    "    del metricdf[\"Shared Transactions Avg\"]\n",
    "    del metricdf[\"L1 Atomic Transactions Avg\"]\n",
    "    del metricdf[\"Local Transactions Avg\"]\n",
    "    del metricdf[\"Global Transactions Avg\"]\n",
    "    \n",
    "    \n",
    "    ### L2 atomic & reduction\n",
    "    metricdf.loc[(metricdf[\"Metric Name\"].str.contains(\"lts__t_sectors_op\")) & (metricdf[\"Metric Type\"]==\"total\"), [\"Metric Value\"]] *= 2\n",
    "    \n",
    "    \n",
    "    ### L2 transactions\n",
    "    # project out\n",
    "    l2df = metricdf.loc[metricdf[\"Metric Name\"].str.contains(\"lts__t_sectors_op\"), selectkeys+[\"Metric Value\"] ].copy()\n",
    "    l2df = l2df.groupby(selectkeys).sum().reset_index().rename(columns={\"Metric Value\": \"L2 Bytes Avg\"})\n",
    "    l2df[\"L2 Bytes Avg\"] *= 32\n",
    "    # add to timings\n",
    "    metricdf = metricdf.merge(l2df[selectkeys+[\"L2 Bytes Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    ### DRAM Bytes\n",
    "    # project out\n",
    "    dramdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"dram__sectors\") ].sort_values(selectkeys)\n",
    "    # get reads and writes\n",
    "    dramreadsdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram__sectors\") & (dramdf[\"Metric Type\"]==\"read\"), selectkeys+[\"Metric Value\"]]\n",
    "    dramwritesdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram__sectors\") & (dramdf[\"Metric Type\"]==\"write\"), selectkeys+[\"Metric Value\"]]\n",
    "    # combine\n",
    "    dramdf = dramwritesdf.merge(dramreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "    dramdf[\"DRAM Transactions\"] = dramdf[\"Metric Value_x\"] + dramdf[\"Metric Value_y\"]\n",
    "    dramdf[\"DRAM Bytes Avg\"] = dramdf[\"DRAM Transactions\"] * 32\n",
    "    metricdf = metricdf.merge(dramdf[selectkeys+[\"DRAM Bytes Avg\"]], on=selectkeys, how=\"inner\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Clean up and return:\n",
    "    del metricdf[\"Metric Value\"]\n",
    "    del metricdf[\"Metric Name\"]\n",
    "    del metricdf[\"Metric Type\"]\n",
    "    #del metricdf[\"Invocations\"]\n",
    "    metricdf.drop_duplicates(keep = 'first', inplace = True)\n",
    "    \n",
    "\n",
    "    ### Get performance\n",
    "    metricdf[\"Performance GFlop/s\"]      = metricdf[\"FLOPs Avg\"]      / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"FP32 Performance GFlop/s\"] = metricdf[\"FP32 FLOPs Avg\"] / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"FP16 Performance GFlop/s\"] = metricdf[\"FP16 FLOPs Avg\"] / (metricdf[\"CUDA Time Avg\"]*10**9)\n",
    "    metricdf[\"TC Performance GFlop/s\"]   = metricdf[\"TC FLOPs Avg\"]   / (metricdf[\"TC Time Avg\"]*10**9)\n",
    "\n",
    "    \n",
    "    ### Get AI\n",
    "    # L1\n",
    "    metricdf[\"L1 AI\"]        = metricdf[\"FLOPs Avg\"]      / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"FP32 L1 AI\"]   = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"FP16 L1 AI\"]   = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"L1 Bytes Avg\"]\n",
    "    metricdf[\"TC L1 AI\"]     = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"L1 Bytes Avg\"]\n",
    "    # L2\n",
    "    metricdf[\"L2 AI\"]        = metricdf[\"FLOPs Avg\"]      / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"FP32 L2 AI\"]   = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"FP16 L2 AI\"]   = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"L2 Bytes Avg\"]\n",
    "    metricdf[\"TC L2 AI\"]     = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"L2 Bytes Avg\"]\n",
    "    # DRAM\n",
    "    metricdf[\"DRAM AI\"]      = metricdf[\"FLOPs Avg\"]      / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"FP32 DRAM AI\"] = metricdf[\"FP32 FLOPs Avg\"] / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"FP16 DRAM AI\"] = metricdf[\"FP16 FLOPs Avg\"] / metricdf[\"DRAM Bytes Avg\"]\n",
    "    metricdf[\"TC DRAM AI\"]   = metricdf[\"TC FLOPs Avg\"]   / metricdf[\"DRAM Bytes Avg\"]\n",
    "\n",
    "\n",
    "    ### Cleanup\n",
    "    metricdf.sort_values(by=selectkeys).reset_index(drop=True, inplace=True)\n",
    "    #print(metricdf[['CUDA Time Avg', 'TC Time Avg']])\n",
    "    \n",
    "    return metricdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the files\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ os.path.join(datadir,x) for x in os.listdir(datadir) if ((os.path.splitext(x)[-1] == \".nsight-cuprof-report\"))]\n",
    "\n",
    "#recs\n",
    "records = []\n",
    "\n",
    "#build feature list:\n",
    "for path in files:\n",
    "    \n",
    "    #filename\n",
    "    file = os.path.basename(path)\n",
    "    \n",
    "    #path\n",
    "    path = os.path.dirname(path)\n",
    "    \n",
    "    #splitup\n",
    "    splt = file.split(\".\")\n",
    "    \n",
    "    prefix = \".\".join(splt[0:-1])\n",
    "    \n",
    "    #append to records\n",
    "    records.append({\"prefix\": prefix, \"file\": os.path.join(path, file)})\n",
    "\n",
    "#put in df\n",
    "recorddf = pd.DataFrame(records).sort_values([\"prefix\"])\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#display(recorddf[\"prefix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by those keys:\n",
    "sortkeys = [\"Network Name\", \"Input Shape\", \"Kernel Shape\", \\\n",
    "            \"Batch Size\", \"Stride Size\", \"Data Format\", \"Pass\", \\\n",
    "            \"Precision\", \"Device\", \"Name\"]\n",
    "\n",
    "#limit the input\n",
    "#recorddf = recorddf[ recorddf[\"prefix\"].str.startswith(\"profile.name_ResNet50-2.batchsize_16.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32\") ]\n",
    "    \n",
    "#group by prefixes and files\n",
    "all_prefixes = set([x.split(\".pass\")[0] for x in recorddf[\"prefix\"]])\n",
    "all_passes = set([x.split(\".pass_\")[1].replace(\".pass_\",\"\") for x in recorddf[\"prefix\"].unique()])\n",
    "\n",
    "#print(recorddf.values[0])\n",
    "\n",
    "#metrics\n",
    "df_profiles = []\n",
    "\n",
    "for pref in all_prefixes:\n",
    "    \n",
    "    #set empty lists\n",
    "    df_times = []\n",
    "    df_timeline = []\n",
    "    df_summary = []\n",
    "    \n",
    "    #print prefix\n",
    "    #print(pref)\n",
    "    \n",
    "    #loop over passes\n",
    "    df_times = []\n",
    "    df_metrics = []\n",
    "    for pas in all_passes:\n",
    "        \n",
    "        #project frame\n",
    "        files = recorddf.loc[ recorddf[\"prefix\"] == pref + \".pass_\" + pas, \"file\" ].values\n",
    "        \n",
    "        #project the invididual files\n",
    "        metricfile = [x for x in files if x.endswith(\".nsight-cuprof-report\")][0]\n",
    "            \n",
    "        #get the parameters from the filename\n",
    "        parameters = parse_filename_nsight(os.path.basename(metricfile))\n",
    "            \n",
    "        #metrics\n",
    "        metricdf = import_nsight_metric(metricfile, cuda_dir=cudadir)\n",
    "        for key in parameters:\n",
    "            metricdf[key] = parameters[key]\n",
    "        \n",
    "        #fuse read/write metrics together:\n",
    "        unique_metrics = metricdf[\"Metric Name\"].unique()\n",
    "        unique_metrics = set([x.replace(\".sum\",\"\").replace(\"_write\",\"\").replace(\"_read\",\"\").replace(\"_ld\",\"\").replace(\"_st\",\"\") for x in unique_metrics])\n",
    "        #add the metric type\n",
    "        metricdf[\"Metric Type\"] = \"total\"\n",
    "        #read\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_read\"), \"Metric Type\" ] = \"read\"\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_ld\"), \"Metric Type\" ] = \"read\"\n",
    "        #write\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_write\"), \"Metric Type\" ] = \"write\"\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\"_st\"), \"Metric Type\" ] = \"write\"\n",
    "        #rate\n",
    "        metricdf.loc[ metricdf[ \"Metric Name\" ].str.contains(\".per_second\"), \"Metric Type\" ] = \"rate\"\n",
    "        \n",
    "        for metric in unique_metrics:\n",
    "            metricdf.loc[ metricdf[ \"Metric Name\"].str.startswith(metric), \"Metric Name\" ] = metric\n",
    "\n",
    "        #append to DF:\n",
    "        df_metrics.append(metricdf)\n",
    "    \n",
    "    metricdf = pd.concat(df_metrics)\n",
    "    \n",
    "    #compute the profile\n",
    "    profiledf = transpose_frame(metricdf)\n",
    "    df_profiles.append(profiledf)\n",
    "\n",
    "#concat everything\n",
    "profiledf = pd.concat(df_profiles)\n",
    "profiledf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(profiledf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute AI Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiledf[ (profiledf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (profiledf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (profiledf[\"Batch Size\"]==16) &\\\n",
    "#           (profiledf[\"Precision\"]==\"FP32\") &\\\n",
    "#           (profiledf[\"Stride Size\"]==2) &\\\n",
    "#           (profiledf[\"Pass\"]==\"forward\") &\\\n",
    "#           (profiledf[\"Kernel Shape\"]==\"7x7x64x64\")\n",
    "#         ]\n",
    "#profiledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum over all kernels\n",
    "combinedselectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \\\n",
    "                     \"Batch Size\", \"Pass\"]\n",
    "\n",
    "#copy profiledf\n",
    "combineddf = profiledf.copy()\n",
    "\n",
    "#get the aggregated performance, including all kernels:\n",
    "#compute weights: multiply all measures by the number of invocations\n",
    "weighted = True\n",
    "if weighted:\n",
    "    #first, get all the names of metrics which need to be weighted\n",
    "    metrics = [x for x in combineddf.columns if \"Avg\" in x]\n",
    "    for metric in metrics:\n",
    "        combineddf[metric] *= combineddf[\"Invocations\"]\n",
    "\n",
    "#sum up\n",
    "combineddf = combineddf.groupby(by=combinedselectkeys).sum().reset_index()\n",
    "\n",
    "\n",
    "#the flop fractions need to be recomputed\n",
    "combineddf[\"FP32 FLOPs Fraction Avg\"] = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"FLOPs Avg\"]\n",
    "combineddf[\"FP16 FLOPs Fraction Avg\"] = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"FLOPs Avg\"]\n",
    "combineddf[\"TC FLOPs Fraction Avg\"]   = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"FLOPs Avg\"]\n",
    "\n",
    "### Get performance\n",
    "combineddf[\"Performance GFlop/s\"]      = combineddf[\"FLOPs Avg\"]      / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"FP32 Performance GFlop/s\"] = combineddf[\"FP32 FLOPs Avg\"] / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"FP16 Performance GFlop/s\"] = combineddf[\"FP16 FLOPs Avg\"] / (combineddf[\"CUDA Time Avg\"]*10**9)\n",
    "combineddf[\"TC Performance GFlop/s\"]   = combineddf[\"TC FLOPs Avg\"]   / (combineddf[\"TC Time Avg\"]*10**9)\n",
    "\n",
    "\n",
    "### Get AI\n",
    "# L1\n",
    "combineddf[\"L1 AI\"]        = combineddf[\"FLOPs Avg\"]      / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"FP32 L1 AI\"]   = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"FP16 L1 AI\"]   = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"L1 Bytes Avg\"]\n",
    "combineddf[\"TC L1 AI\"]     = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"L1 Bytes Avg\"]\n",
    "# L2\n",
    "combineddf[\"L2 AI\"]        = combineddf[\"FLOPs Avg\"]      / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"FP32 L2 AI\"]   = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"FP16 L2 AI\"]   = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"L2 Bytes Avg\"]\n",
    "combineddf[\"TC L2 AI\"]     = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"L2 Bytes Avg\"]\n",
    "# DRAM\n",
    "combineddf[\"DRAM AI\"]      = combineddf[\"FLOPs Avg\"]      / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"FP32 DRAM AI\"] = combineddf[\"FP32 FLOPs Avg\"] / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"FP16 DRAM AI\"] = combineddf[\"FP16 FLOPs Avg\"] / combineddf[\"DRAM Bytes Avg\"]\n",
    "combineddf[\"TC DRAM AI\"]   = combineddf[\"TC FLOPs Avg\"]   / combineddf[\"DRAM Bytes Avg\"]\n",
    "\n",
    "combineddf.sort_values(by=combinedselectkeys).reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Precision', 'Network Name', 'Data Format', 'Input Shape',\n",
       "       'Kernel Shape', 'Stride Size', 'Batch Size', 'Pass', 'Invocations',\n",
       "       'CUDA Time Avg', 'TC Time Avg', 'FP32 FLOPs Avg', 'FP16 FLOPs Avg',\n",
       "       'TC FLOPs Avg', 'FLOPs Avg', 'FP32 FLOPs Fraction Avg',\n",
       "       'FP16 FLOPs Fraction Avg', 'TC FLOPs Fraction Avg', 'L1 Bytes Avg',\n",
       "       'L2 Bytes Avg', 'DRAM Bytes Avg', 'Performance GFlop/s',\n",
       "       'FP32 Performance GFlop/s', 'FP16 Performance GFlop/s',\n",
       "       'TC Performance GFlop/s', 'L1 AI', 'FP32 L1 AI', 'FP16 L1 AI',\n",
       "       'TC L1 AI', 'L2 AI', 'FP32 L2 AI', 'FP16 L2 AI', 'TC L2 AI', 'DRAM AI',\n",
       "       'FP32 DRAM AI', 'FP16 DRAM AI', 'TC DRAM AI'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUDA Time Avg</th>\n",
       "      <th>FP32 FLOPs Avg</th>\n",
       "      <th>FP16 FLOPs Avg</th>\n",
       "      <th>TC FLOPs Avg</th>\n",
       "      <th>L1 Bytes Avg</th>\n",
       "      <th>L2 Bytes Avg</th>\n",
       "      <th>DRAM Bytes Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055706</td>\n",
       "      <td>9.152922e+10</td>\n",
       "      <td>3.189801e+10</td>\n",
       "      <td>1.860795e+12</td>\n",
       "      <td>1.868912e+12</td>\n",
       "      <td>1.460629e+12</td>\n",
       "      <td>9.324507e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011972</td>\n",
       "      <td>5.138022e+09</td>\n",
       "      <td>1.085932e+10</td>\n",
       "      <td>2.043442e+11</td>\n",
       "      <td>1.782225e+11</td>\n",
       "      <td>1.637744e+11</td>\n",
       "      <td>1.052987e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUDA Time Avg  FP32 FLOPs Avg  FP16 FLOPs Avg  TC FLOPs Avg  L1 Bytes Avg  \\\n",
       "0       0.055706    9.152922e+10    3.189801e+10  1.860795e+12  1.868912e+12   \n",
       "1       0.011972    5.138022e+09    1.085932e+10  2.043442e+11  1.782225e+11   \n",
       "\n",
       "   L2 Bytes Avg  DRAM Bytes Avg  \n",
       "0  1.460629e+12    9.324507e+11  \n",
       "1  1.637744e+11    1.052987e+11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combineddf.columns)\n",
    "display(combineddf[['CUDA Time Avg', 'FP32 FLOPs Avg', 'FP16 FLOPs Avg', 'TC FLOPs Avg', 'L1 Bytes Avg', 'L2 Bytes Avg', 'DRAM Bytes Avg']])\n",
    "# combineddf.keys\n",
    "# combineddf.columns\n",
    "\n",
    "# combineddf['Name']\n",
    "# combineddf.iloc[0,1]\n",
    "# combineddf.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L2 AI</th>\n",
       "      <th>L1 AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.358471</td>\n",
       "      <td>1.061699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.345397</td>\n",
       "      <td>1.236328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      L2 AI     L1 AI\n",
       "0  1.358471  1.061699\n",
       "1  1.345397  1.236328"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combineddf = combineddf.reset_index()\n",
    "#seldf = combineddf[ (combineddf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (combineddf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (combineddf[\"Precision\"]==\"FP32\")]\n",
    "#seldf\n",
    "#combineddf[[\"FP32 L2 AI\", \"FP32 L1 AI\"]]\n",
    "combineddf[[\"L2 AI\", \"L1 AI\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiledf.to_csv(os.path.join(outputdir,\"full_profile.csv\"))\n",
    "combineddf.to_csv(os.path.join(outputdir,\"combined_profile.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
