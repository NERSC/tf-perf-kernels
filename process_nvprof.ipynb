{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters\n",
    "cudadir = \"/global/common/cori/software/cuda/10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \n",
    "    #empty dicts\n",
    "    result={}\n",
    "    \n",
    "    #add network name\n",
    "    result[\"Network Name\"] = re.match(r'.*\\.name_(.*?)\\.',file).groups()[0]\n",
    "    result[\"Batch Size\"] = int(re.match(r'.*\\.batchsize_(.*?)\\.',file).groups()[0])\n",
    "    result[\"Input Shape\"] = re.match(r'.*\\.inputshape_(.*?)\\.',file).groups()[0]\n",
    "    result[\"Kernel Shape\"] = re.match(r'.*\\.kernelshape_(.*?)\\.',file).groups()[0]\n",
    "    result[\"Stride Size\"] = int(re.match(r'.*\\.stride_(.*?)\\.',file).groups()[0])\n",
    "    result[\"Data Format\"] = re.match(r'.*\\.dataformat_(.*?)\\.',file).groups()[0]\n",
    "    result[\"Pass\"] = re.match(r'.*\\.pass_(.*?)\\.',file).groups()[0]\n",
    "    prec = int(re.match(r'.*\\.fp(.*?)\\.',file).groups()[0])\n",
    "    result[\"Precision\"] = \"FP16\" if prec==16 else \"FP32\";\n",
    "    metric = re.match(r'.*\\.metric_(.*?)\\.',file).groups()[0]\n",
    "    \n",
    "    return result, metric\n",
    "\n",
    "\n",
    "def import_nvprof_metric(filename, timeline=False):\n",
    "    #execute nvprof and parse file\n",
    "    args = [os.path.join(cudadir, \"bin/nvprof\"),\"--csv\",\"-i\",filename]\n",
    "    skiprows = 2\n",
    "    \n",
    "    #if timeline is enabled, we have to skip less rows also\n",
    "    if timeline:\n",
    "        args.append(\"--print-gpu-trace\")\n",
    "        skiprows = 1\n",
    "    \n",
    "    #open subprocess and communicate\n",
    "    p = sp.Popen(args, stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "\n",
    "    #get timeline from csv\n",
    "    profiledf = pd.read_csv(StringIO(stderr.decode(\"utf-8\")),skiprows=skiprows).dropna(how=\"all\").rename(columns={\"Kernel\": \"Name\"})\n",
    "    profiledf[\"Collection Type\"] = \"kernel\"\n",
    "    \n",
    "    #return result\n",
    "    return profiledf\n",
    "\n",
    "\n",
    "def import_nvprof_overview(filename, nvtx=False):\n",
    "    #execute nvprof and parse file\n",
    "    args = [os.path.join(cudadir, \"bin/nvprof\"),\"--csv\",\"-i\",filename]\n",
    "    \n",
    "    #open subprocess and communicate\n",
    "    p = sp.Popen(args, stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "    stdout, stderr = p.communicate()\n",
    "\n",
    "    #now remove the ranges\n",
    "    inp = stderr.decode(\"utf-8\")\n",
    "    \n",
    "    #get the profiling data\n",
    "    profile = inp.split(\"======== NVTX result\")[0]\n",
    "    \n",
    "    if nvtx:\n",
    "        marker = inp.split(\"======== NVTX result\")[1]\n",
    "\n",
    "    #we can readily use the profile info\n",
    "    profiledf = pd.read_csv(StringIO(profile), skiprows=1, header=[0,1]).dropna(how=\"all\")\n",
    "    \n",
    "    #make the time units the same:\n",
    "    for col in profiledf.columns:\n",
    "        if col[1] == \"ms\":\n",
    "            profiledf[col] *= 10**(-3)\n",
    "        elif col[1] == \"us\":\n",
    "            profiledf[col] *= 10**(-6)\n",
    "        elif col[1] == \"ns\":\n",
    "            profiledf[col] *= 10**(-9)\n",
    "            \n",
    "    #now drop that header\n",
    "    profiledf.columns = profiledf.columns.droplevel(1)\n",
    "    \n",
    "    #now sort\n",
    "    profiledf = profiledf.sort_values(by=[\"Type\", \"Name\"]).reset_index(drop=True)\n",
    "    profiledf[\"Metric Name\"] = \"time\"\n",
    "    \n",
    "    #some renamings\n",
    "    profiledf.loc[ profiledf[\"Type\"] == \"GPU activities\", \"Type\" ] = \"gpu_activities\"\n",
    "    profiledf.loc[ profiledf[\"Type\"] == \"API calls\", \"Type\" ] = \"api_calls\"\n",
    "    \n",
    "    #rename columns\n",
    "    profiledf.rename(columns={\"Type\": \"Collection Type\"}, inplace=True)\n",
    "    \n",
    "    if nvtx:\n",
    "        markerdflist = []\n",
    "        for it in re.finditer(r\"========\\s{1,}Range(.*?)(==|$)\", marker, flags=re.DOTALL):\n",
    "            #read into DF\n",
    "            tmpdf = pd.read_csv(StringIO(it.groups()[0]),skiprows=lambda x: x in [0,2], header=0)\n",
    "            del tmpdf[\"Time(%)\"]\n",
    "    \n",
    "            #drop rows without info\n",
    "            tmpdf = tmpdf[ ~tmpdf[\"Type\"].str.contains(\"were profiled in this range\") ]\n",
    "    \n",
    "            #extract range name:\n",
    "            rangename = tmpdf.loc[ tmpdf[\"Type\"] == \"Range:\", \"Name\" ][0]\n",
    "        \n",
    "            #some renamings\n",
    "            tmpdf.loc[ tmpdf[\"Type\"] == \"Range:\", \"Name\" ] = \"total\"\n",
    "            tmpdf.loc[ tmpdf[\"Type\"] == \"Range:\", \"Type\" ] = \"range\"\n",
    "            tmpdf.loc[ tmpdf[\"Type\"] == \"GPU activities\", \"Type\" ] = \"gpu_activities\"\n",
    "            tmpdf.loc[ tmpdf[\"Type\"] == \"API calls\", \"Type\" ] = \"api_calls\"\n",
    "    \n",
    "            #add the rangename to the entries\n",
    "            tmpdf[\"Range Name\"] = rangename\n",
    "    \n",
    "            #renaming\n",
    "            tmpdf.rename(columns={\"Type\": \"Collection Type\"}, inplace=True)\n",
    "    \n",
    "            #add to list\n",
    "            markerdflist.append(tmpdf)\n",
    "    \n",
    "        #concat the crap\n",
    "        markerdf = pd.concat(markerdflist).sort_values(by=[\"Range Name\", \"Time\"], ascending=[True, False]).reset_index(drop=True)\n",
    "    else:\n",
    "        markerdf = pd.DataFrame()\n",
    "    \n",
    "    return profiledf, markerdf\n",
    "\n",
    "def combine_metrics(df, metrics):\n",
    "    return pd.DataFrame.from_records([{\"Metric Count\": df[m].values[0], \"Metric Name\": m.replace(\"read\",\"\").replace(\"write\",\"\").replace(\"__\",\"_\"), \\\n",
    "    \"Metric Mode\": \"read\" if \"read\" in m else \"write\" if \"write\" in m else \"total\"} for m in metrics])\n",
    "\n",
    "\n",
    "def replace_tc_string(value):\n",
    "    value = int(re.match(r\".*?\\((.*?)\\)\",value).groups()[0])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_32.inputshape_112x112x64.kernelshape_9x9x64x64.stride_3.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x128.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x256.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x512.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x64.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x64.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_3x3x64x64.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_2.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_7x7x64x64.stride_3.dataformat_NHWC.fp16.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp32.pass_forward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_backward',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_calibrate',\n",
       " 'name_ResNet50-2.batchsize_64.inputshape_112x112x64.kernelshape_9x9x64x64.stride_2.dataformat_NHWC.fp32.pass_forward']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#datadir:\n",
    "datadirs = [\"./data/good_new\", \"./data/good_new_2\"]\n",
    "\n",
    "#get metric list\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ x for x in os.listdir(datadir) if (os.path.splitext(x)[-1] == \".nvprof\") or (os.path.splitext(x)[-1] == \".nvvp\") ]\n",
    "\n",
    "#recs\n",
    "records = []\n",
    "\n",
    "#build feature list:\n",
    "for file in files:\n",
    "    splt = file.split(\".\")\n",
    "    \n",
    "    prefix = \".\".join(splt[1:-2])\n",
    "    metric = splt[-2].split(\"metric_\")[1]\n",
    "    \n",
    "    #append to records\n",
    "    records.append({\"prefix\": prefix, \"metric\": metric})\n",
    "\n",
    "#put in df\n",
    "recorddf = pd.DataFrame(records).sort_values([\"prefix\", \"metric\"])\n",
    "\n",
    "#get all metrics\n",
    "all_metrics = list(recorddf[\"metric\"].unique())\n",
    "\n",
    "#group by metric:\n",
    "missingrecorddf = pd.DataFrame(recorddf.groupby(\"prefix\").apply(lambda x: pd.Series([y for y in all_metrics if y not in list(x[\"metric\"])])))\n",
    "\n",
    "#create exclusion list:\n",
    "excludelist = list(missingrecorddf.reset_index()[\"prefix\"].unique())\n",
    "\n",
    "#print the missing ones\n",
    "excludelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datadir:\n",
    "datadirs = [\"./data/good_new\", \"./data/good_new_2\"]\n",
    "#datadir = \"/global/cscratch1/sd/tkurth/tf_cnn_kernels/runs/65709\"\n",
    "#datadir = \"/global/cscratch1/sd/tkurth/tf_cnn_kernels/runs/70633\"\n",
    "\n",
    "#sort by those:\n",
    "sortkeys = [\"Network Name\", \"Input Shape\", \"Kernel Shape\", \\\n",
    "            \"Batch Size\", \"Stride Size\", \"Data Format\", \"Pass\", \\\n",
    "            \"Precision\", \"Device\", \"Name\", \"Metric Name\"]\n",
    "\n",
    "#init lists to zero\n",
    "df_timeline = []\n",
    "df_summary = []\n",
    "df_summary_derived = []\n",
    "\n",
    "#get metric list\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ os.path.join(datadir,x) for x in os.listdir(datadir) if (os.path.splitext(x)[-1] == \".nvprof\") or (os.path.splitext(x)[-1] == \".nvvp\") ]\n",
    "\n",
    "##DEBUG\n",
    "#files = [x for x in files if x.startswith(\"profile.name_ResNet50-2.batchsize_16.inputshape_112x112x64.kernelshape_9x9x64x64.stride_1.dataformat_NHWC.fp16.pass_forward.metric_\")]\n",
    "##DEBUG\n",
    "\n",
    "#metrics\n",
    "for file in files:\n",
    "    \n",
    "    #extract metric name\n",
    "    parameters, metric = parse_filename(os.path.basename(file))\n",
    "    metrics = metric.split(\"-\")\n",
    "    \n",
    "    #print(\"Reading {}\".format(file))\n",
    "    \n",
    "    if metric == \"time\":\n",
    "        continue\n",
    "\n",
    "    #import as timeline\n",
    "    tmpdf = import_nvprof_metric(file, timeline=True)\n",
    "    for key in parameters:\n",
    "        tmpdf[key] = parameters[key]\n",
    "        \n",
    "    #replace \"Idle (0)\" with 0.:\n",
    "    for metric in metrics:\n",
    "        if metric==\"tensor_precision_fu_utilization\":\n",
    "            tmpdf[metric] = tmpdf[metric].apply(lambda x: replace_tc_string(x))\n",
    "    \n",
    "    #combine read and write metrics\n",
    "    tmpdf = tmpdf.groupby([x for x in tmpdf.columns if x not in metrics]).apply(lambda x: combine_metrics(x, metrics)).reset_index()\n",
    "    lev = [x for x in tmpdf.columns if x.startswith(\"level_\")][0]\n",
    "    del tmpdf[lev]\n",
    "    df_timeline.append(tmpdf)\n",
    "    \n",
    "    #import as summary\n",
    "    tmpdf = import_nvprof_metric(file, timeline=False).sort_values(by=\"Name\").reset_index(drop=True)\n",
    "    tmpdf[\"Metric Mode\"] = \"read\" if \"read\" in metric else \"write\" if \"write\" in metric else \"write\" if \"store\" in metric else \"total\"\n",
    "    tmpdf[\"Metric Name\"] = metric.replace(\"read\",\"\").replace(\"write\",\"\").replace(\"store\",\"\").replace(\"__\",\"_\")\n",
    "    for key in parameters:\n",
    "        tmpdf[key] = parameters[key]\n",
    "    del tmpdf[\"Metric Description\"]\n",
    "    \n",
    "    #replace \"Idle (0)\" with 0.:\n",
    "    for metric in metrics:\n",
    "        if metric==\"tensor_precision_fu_utilization\":\n",
    "            tmpdf[ \"Min\" ] = tmpdf[ \"Min\" ].apply(lambda x: replace_tc_string(x))\n",
    "            tmpdf[ \"Max\" ] = tmpdf[ \"Max\" ].apply(lambda x: replace_tc_string(x))\n",
    "            tmpdf[ \"Avg\" ] = tmpdf[ \"Avg\" ].apply(lambda x: replace_tc_string(x))\n",
    "    df_summary.append(tmpdf)\n",
    "\n",
    "#concat the frames\n",
    "df_timeline = pd.concat(df_timeline, sort=True)\n",
    "df_timeline = df_timeline.sort_values(by=sortkeys+[\"Metric Mode\", \"Correlation_ID\"]).reset_index(drop=True)\n",
    "df_summary = pd.concat(df_summary, sort=True)\n",
    "\n",
    "#compute summary df:\n",
    "tmpdf = df_timeline.groupby([x for x in df_timeline.columns if x not in [\"Metric Mode\", \"Metric Count\"]]).apply(lambda x: pd.Series({\"Metric Count\": x[\"Metric Count\"].values.sum(), \"Metric Mode\": \"total\"})).reset_index()\n",
    "tmpdf.sort_values(sortkeys+[\"Correlation_ID\"]).reset_index(drop=True, inplace=True)\n",
    "df_summary_derived = tmpdf.groupby(sortkeys).apply(lambda x: pd.Series({\"Invocations\": x[\"Metric Count\"].count(), \\\n",
    "                                                               \"Min\": x[\"Metric Count\"].min(), \\\n",
    "                                                               \"Max\": x[\"Metric Count\"].max(), \\\n",
    "                                                               \"STD\": x[\"Metric Count\"].std(), \\\n",
    "                                                               \"Average\": x[\"Metric Count\"].mean()})).reset_index().sort_values(by=\"Name\").fillna(0.)\n",
    "\n",
    "#timings\n",
    "files = []\n",
    "for datadir in datadirs:\n",
    "    files += [ os.path.join(datadir,x) for x in os.listdir(datadir) if ((os.path.splitext(x)[-1] == \".nvprof\") or (os.path.splitext(x)[-1] == \".nvvp\")) and \"metric_time\" in x ]\n",
    "\n",
    "df_times = []\n",
    "for file in files:\n",
    "    timedf, markerdf = import_nvprof_overview(os.path.basename(file))\n",
    "    \n",
    "    #extract metric name\n",
    "    parameters, _ = parse_filename(file)\n",
    "    for key in parameters:\n",
    "        timedf[key] = parameters[key]\n",
    "    \n",
    "    #append frame\n",
    "    df_times.append(timedf)\n",
    "    \n",
    "df_time = pd.concat(df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute AI Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the profile frame to make sure not to overwrite it and potentially read it in again if we screwed it up\n",
    "selectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \\\n",
    "             \"Batch Size\", \"Pass\", \"Name\"]\n",
    "tc_peak_perf_flops = 125*10**12\n",
    "\n",
    "#just pick the gpu activities for now\n",
    "profiledf = df_time[ df_time[\"Collection Type\"] == \"gpu_activities\" ].copy()\n",
    "profiledf.sort_values(by=[\"Name\"],inplace=True)\n",
    "profiledf.reset_index(drop=True, inplace=True)\n",
    "profiledf.rename(columns={\"Avg\": \"Time Avg\"}, inplace=True)\n",
    "del profiledf[\"Time(%)\"]\n",
    "del profiledf[\"Time\"]\n",
    "del profiledf[\"Min\"]\n",
    "del profiledf[\"Max\"]\n",
    "del profiledf[\"Metric Name\"]\n",
    "del profiledf[\"Collection Type\"]\n",
    "\n",
    "#remove the calibration\n",
    "alignkeys = selectkeys[:-2]\n",
    "profiledf = profiledf.groupby(alignkeys).apply(lambda x: x[ (~x[\"Name\"].isin(x.loc[x[\"Pass\"].str.startswith(\"calibrate\"), \"Name\"].values)) ])\n",
    "profiledf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#as metricdf use df_summary\n",
    "metricdf = df_summary.copy()\n",
    "\n",
    "#now, get the AI-relevant stuff:\n",
    "#FLOPS 32\n",
    "flopdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"flop_count_sp\") ].sort_values(selectkeys).rename(columns={\"Avg\": \"FP32 Flops Avg\"})\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(flopdf[selectkeys+[\"FP32 Flops Avg\"]], on=selectkeys, how=\"left\")\n",
    "\n",
    "#FLOPS 16 non-TC\n",
    "flopdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"flop_count_hp\") ].sort_values(selectkeys).rename(columns={\"Avg\": \"FP16 non-TC Flops Avg\"})\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(flopdf[selectkeys+[\"FP16 non-TC Flops Avg\"]], on=selectkeys, how=\"left\")\n",
    "\n",
    "#FLOPS TC\n",
    "flopdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"tensor_precision_fu_utilization\") ].sort_values(selectkeys).rename(columns={\"Avg\": \"TC Flops Avg\"})\n",
    "tmpdf = flopdf.merge(profiledf, how=\"inner\", on=selectkeys).sort_values(selectkeys)\n",
    "tmpdf[\"TC Flops Avg\"] *= tc_peak_perf_flops/10. * tmpdf[\"Time Avg\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(tmpdf[selectkeys+[\"TC Flops Avg\"]], on=selectkeys, how=\"left\")\n",
    "\n",
    "#fill NA values here\n",
    "profiledf.fillna(0., inplace=True)\n",
    "\n",
    "#FLOPS FP16: add TC and non-TC FP16 flops together\n",
    "profiledf[\"FP16 Flops Avg\"] = profiledf[\"TC Flops Avg\"] + profiledf[\"FP16 non-TC Flops Avg\"]\n",
    "\n",
    "#total flops\n",
    "profiledf[\"Flops Avg\"] = profiledf[\"FP16 Flops Avg\"] + profiledf[\"FP32 Flops Avg\"]\n",
    "\n",
    "#flop fractions\n",
    "profiledf[\"TC Flops Fraction Avg\"] = profiledf[\"TC Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "profiledf[\"FP16 Flops Fraction Avg\"] = profiledf[\"FP16 Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "profiledf[\"FP16 non-TC Flops Fraction Avg\"] = profiledf[\"FP16 non-TC Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "profiledf[\"FP32 Flops Fraction Avg\"] = profiledf[\"FP32 Flops Avg\"]/profiledf[\"Flops Avg\"]\n",
    "\n",
    "\n",
    "#shared\n",
    "#project out\n",
    "shareddf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"shared\") ].sort_values(selectkeys)\n",
    "#get reads and writes\n",
    "sharedreadsdf = shareddf.loc[(shareddf[\"Metric Name\"]==\"shared_transactions\") & (shareddf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "sharedwritesdf = shareddf.loc[(shareddf[\"Metric Name\"]==\"shared_transactions\") & (shareddf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "#combine\n",
    "shareddf = sharedwritesdf.merge(sharedreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "shareddf[\"Shared Transactions Avg\"] = shareddf[\"Avg_x\"] + shareddf[\"Avg_y\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(shareddf[selectkeys+[\"Shared Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "\n",
    "#L1\n",
    "#project out\n",
    "l1df = metricdf[ (metricdf[\"Metric Name\"].str.contains(\"gst_\")) | (metricdf[\"Metric Name\"].str.contains(\"gld_\")) ].sort_values(selectkeys)\n",
    "#get reads and writes\n",
    "l1readsdf = l1df.loc[(l1df[\"Metric Name\"]==\"gld_transactions\"), selectkeys+[\"Avg\"]]\n",
    "l1writesdf = l1df.loc[(l1df[\"Metric Name\"]==\"gst_transactions\"), selectkeys+[\"Avg\"]]\n",
    "#combine\n",
    "l1df = l1writesdf.merge(l1readsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "l1df[\"L1 Transactions Avg\"] = l1df[\"Avg_x\"] + l1df[\"Avg_y\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(l1df[selectkeys+[\"L1 Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "\n",
    "#L2\n",
    "#project out\n",
    "l2df = metricdf[ metricdf[\"Metric Name\"].str.contains(\"l2\") ].sort_values(selectkeys)\n",
    "#get reads and writes\n",
    "l2readsdf = l2df.loc[(l2df[\"Metric Name\"]==\"l2_transactions\") & (l2df[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "l2writesdf = l2df.loc[(l2df[\"Metric Name\"]==\"l2_transactions\") & (l2df[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "#combine\n",
    "l2df = l2writesdf.merge(l2readsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "l2df[\"L2 Transactions Avg\"] = l2df[\"Avg_x\"] + l2df[\"Avg_y\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(l2df[selectkeys+[\"L2 Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "\n",
    "#DRAM\n",
    "#project out\n",
    "dramdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"dram\") ].sort_values(selectkeys)\n",
    "#get reads and writes\n",
    "dramreadsdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram_transactions\") & (dramdf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "dramwritesdf = dramdf.loc[(dramdf[\"Metric Name\"]==\"dram_transactions\") & (dramdf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "#combine\n",
    "dramdf = dramwritesdf.merge(dramreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "dramdf[\"DRAM Transactions Avg\"] = dramdf[\"Avg_x\"] + dramdf[\"Avg_y\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(dramdf[selectkeys+[\"DRAM Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "\n",
    "#SYSMEM\n",
    "#project out\n",
    "sysmemdf = metricdf[ metricdf[\"Metric Name\"].str.contains(\"sysmem\") ].sort_values(selectkeys)\n",
    "#get reads and writes\n",
    "sysmemreadsdf = sysmemdf.loc[(sysmemdf[\"Metric Name\"]==\"sysmem_transactions\") & (sysmemdf[\"Metric Mode\"]==\"read\"), selectkeys+[\"Avg\"]]\n",
    "sysmemwritesdf = sysmemdf.loc[(sysmemdf[\"Metric Name\"]==\"sysmem_transactions\") & (sysmemdf[\"Metric Mode\"]==\"write\"), selectkeys+[\"Avg\"]]\n",
    "#combine\n",
    "sysmemdf = sysmemwritesdf.merge(sysmemreadsdf, on=selectkeys, how=\"outer\").fillna(0.)\n",
    "sysmemdf[\"Sysmem Transactions Avg\"] = sysmemdf[\"Avg_x\"] + sysmemdf[\"Avg_y\"]\n",
    "#add to timings\n",
    "profiledf = profiledf.merge(sysmemdf[selectkeys+[\"Sysmem Transactions Avg\"]], on=selectkeys, how=\"inner\")\n",
    "\n",
    "#clean up and sort:\n",
    "profiledf.sort_values(selectkeys).reset_index(drop=True, inplace=True)\n",
    "\n",
    "#get performance first\n",
    "profiledf[\"Performance GFlop/s\"] = profiledf[\"Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "profiledf[\"FP32 Performance GFlop/s\"] = profiledf[\"FP32 Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "profiledf[\"FP16 Performance GFlop/s\"] = profiledf[\"FP16 Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "profiledf[\"TC Performance GFlop/s\"] = profiledf[\"TC Flops Avg\"]/(profiledf[\"Time Avg\"]*10**9)\n",
    "\n",
    "#get AI:\n",
    "#L1 is L1+shared\n",
    "profiledf[\"L1 AI\"] = profiledf[\"Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]))\n",
    "profiledf[\"FP32 L1 AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]))\n",
    "profiledf[\"FP16 L1 AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*(profiledf[\"L1 Transactions Avg\"]+profiledf[\"Shared Transactions Avg\"]))\n",
    "#L2\n",
    "profiledf[\"L2 AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "profiledf[\"FP32 L2 AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "profiledf[\"FP16 L2 AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"L2 Transactions Avg\"])\n",
    "#DRAM\n",
    "profiledf[\"DRAM AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "profiledf[\"FP32 DRAM AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "profiledf[\"FP16 DRAM AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"DRAM Transactions Avg\"])\n",
    "#Sysmem\n",
    "profiledf[\"Sysmem AI\"] = profiledf[\"Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "profiledf[\"FP32 Sysmem AI\"] = profiledf[\"FP32 Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "profiledf[\"FP16 Sysmem AI\"] = profiledf[\"FP16 Flops Avg\"]/(32.*profiledf[\"Sysmem Transactions Avg\"])\n",
    "\n",
    "#sort results\n",
    "profiledf.sort_values(by=selectkeys).reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiledf[ (profiledf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (profiledf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (profiledf[\"Batch Size\"]==16) &\\\n",
    "#           (profiledf[\"Precision\"]==\"FP16\") &\\\n",
    "#           (profiledf[\"Stride Size\"]==3) &\\\n",
    "#           (profiledf[\"Pass\"]==\"backward\") &\\\n",
    "#           (profiledf[\"Kernel Shape\"]==\"9x9x64x64\")\n",
    "#         ]\n",
    "profiledf.loc[ profiledf[\"Pass\"]==\"backward\", [\"L1 AI\", \"L2 AI\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum over all kernels\n",
    "combinedselectkeys = [\"Precision\", \"Network Name\", \"Data Format\", \"Input Shape\", \"Kernel Shape\", \"Stride Size\", \\\n",
    "                     \"Batch Size\", \"Pass\"]\n",
    "\n",
    "#get the aggregated performance, including all kernels:\n",
    "combineddf = profiledf.groupby(by=combinedselectkeys).sum()\n",
    "\n",
    "#the flop fractions need to be recomputed\n",
    "combineddf[\"TC Flops Fraction Avg\"] = combineddf[\"TC Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP16 Flops Fraction Avg\"] = combineddf[\"FP16 Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP16 non-TC Flops Fraction Avg\"] = combineddf[\"FP16 non-TC Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "combineddf[\"FP32 Flops Fraction Avg\"] = combineddf[\"FP32 Flops Avg\"]/combineddf[\"Flops Avg\"]\n",
    "\n",
    "#get performance first\n",
    "combineddf[\"Performance GFlop/s\"] = combineddf[\"Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"FP32 Performance GFlop/s\"] = combineddf[\"FP32 Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"FP16 Performance GFlop/s\"] = combineddf[\"FP16 Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "combineddf[\"TC Performance GFlop/s\"] = combineddf[\"TC Flops Avg\"]/(combineddf[\"Time Avg\"]*10**9)\n",
    "\n",
    "#get AI:\n",
    "#L1 is L1+shared\n",
    "combineddf[\"L1 AI\"] = combineddf[\"Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]))\n",
    "combineddf[\"FP32 L1 AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]))\n",
    "combineddf[\"FP16 L1 AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]))\n",
    "combineddf[\"TC L1 AI\"] = combineddf[\"TC Flops Avg\"]/(32.*(combineddf[\"L1 Transactions Avg\"]+combineddf[\"Shared Transactions Avg\"]))\n",
    "#L2\n",
    "combineddf[\"L2 AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"FP32 L2 AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"FP16 L2 AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "combineddf[\"TC L2 AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"L2 Transactions Avg\"])\n",
    "#DRAM\n",
    "combineddf[\"DRAM AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"FP32 DRAM AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"FP16 DRAM AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "combineddf[\"TC DRAM AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"DRAM Transactions Avg\"])\n",
    "#Sysmem\n",
    "combineddf[\"Sysmem AI\"] = combineddf[\"Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"FP32 Sysmem AI\"] = combineddf[\"FP32 Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"FP16 Sysmem AI\"] = combineddf[\"FP16 Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "combineddf[\"TC Sysmem AI\"] = combineddf[\"TC Flops Avg\"]/(32.*combineddf[\"Sysmem Transactions Avg\"])\n",
    "\n",
    "#print\n",
    "combineddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineddf = combineddf.reset_index()\n",
    "#seldf = combineddf[ (combineddf[\"Network Name\"]==\"ResNet50-2\") &\\\n",
    "#           (combineddf[\"Input Shape\"]==\"112x112x64\") &\\\n",
    "#           (combineddf[\"Precision\"]==\"FP32\")]\n",
    "#seldf\n",
    "#combineddf[[\"FP32 L2 AI\", \"FP32 L1 AI\"]]\n",
    "combineddf[[\"L2 Transactions Avg\", \"L1 Transactions Avg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"./results\"\n",
    "\n",
    "profiledf.to_csv(os.path.join(outputdir,\"full_profile.csv\"))\n",
    "combineddf.to_csv(os.path.join(outputdir,\"combined_profile.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.11 py36",
   "language": "python",
   "name": "tensorflow_1.11.0_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
